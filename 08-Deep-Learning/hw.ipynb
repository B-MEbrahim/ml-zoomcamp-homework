{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\n\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:07:37.554675Z","iopub.execute_input":"2025-12-01T13:07:37.554958Z","iopub.status.idle":"2025-12-01T13:07:41.132464Z","shell.execute_reply.started":"2025-12-01T13:07:37.554936Z","shell.execute_reply":"2025-12-01T13:07:41.131865Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n!unzip data.zip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms, datasets\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:07:53.937638Z","iopub.execute_input":"2025-12-01T13:07:53.938401Z","iopub.status.idle":"2025-12-01T13:07:57.334353Z","shell.execute_reply.started":"2025-12-01T13:07:53.938362Z","shell.execute_reply":"2025-12-01T13:07:57.333717Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:07:58.777077Z","iopub.execute_input":"2025-12-01T13:07:58.777843Z","iopub.status.idle":"2025-12-01T13:07:58.783317Z","shell.execute_reply.started":"2025-12-01T13:07:58.777820Z","shell.execute_reply":"2025-12-01T13:07:58.782589Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"class BaseModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2)\n        self.fc1 = nn.Linear(32 * 100 * 100, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        out = torch.relu(self.conv1(x))\n        out = self.pool(out)\n        out = out.view(-1, 32 * 100 * 100)\n        out = torch.relu(self.fc1(out))\n        out = self.fc2(out)  \n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:08:01.102666Z","iopub.execute_input":"2025-12-01T13:08:01.103234Z","iopub.status.idle":"2025-12-01T13:08:01.108282Z","shell.execute_reply.started":"2025-12-01T13:08:01.103212Z","shell.execute_reply":"2025-12-01T13:08:01.107487Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = BaseModel().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:08:02.146651Z","iopub.execute_input":"2025-12-01T13:08:02.147528Z","iopub.status.idle":"2025-12-01T13:08:02.510994Z","shell.execute_reply.started":"2025-12-01T13:08:02.147493Z","shell.execute_reply":"2025-12-01T13:08:02.510209Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:08:03.296041Z","iopub.execute_input":"2025-12-01T13:08:03.296313Z","iopub.status.idle":"2025-12-01T13:08:03.301061Z","shell.execute_reply.started":"2025-12-01T13:08:03.296291Z","shell.execute_reply":"2025-12-01T13:08:03.300483Z"}},"outputs":[{"name":"stdout","text":"Total parameters: 20481025\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.Resize((200, 200)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ) # ImageNet normalization\n])\ntest_transforms = transforms.Compose([\n    transforms.Resize((200, 200)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ) # ImageNet normalization\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:08:04.874897Z","iopub.execute_input":"2025-12-01T13:08:04.875169Z","iopub.status.idle":"2025-12-01T13:08:04.882755Z","shell.execute_reply.started":"2025-12-01T13:08:04.875147Z","shell.execute_reply":"2025-12-01T13:08:04.882015Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"data_dir = '/kaggle/working/data'\n\nclass HairDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        self.classes = sorted(os.listdir(data_dir))\n        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n\n        for label_name in self.classes:\n            label_dir = os.path.join(data_dir, label_name)\n            for img_name in os.listdir(label_dir):\n                self.image_paths.append(os.path.join(label_dir, img_name))\n                self.labels.append(self.class_to_idx[label_name])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:08:06.567916Z","iopub.execute_input":"2025-12-01T13:08:06.568204Z","iopub.status.idle":"2025-12-01T13:08:06.574877Z","shell.execute_reply.started":"2025-12-01T13:08:06.568180Z","shell.execute_reply":"2025-12-01T13:08:06.573930Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataset = HairDataset(\n    data_dir = '/kaggle/working/data/train',\n    transform=train_transforms\n)\ntest_dataset = HairDataset(\n    data_dir = '/kaggle/working/data/test',\n    transform=test_transforms\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:08:08.589969Z","iopub.execute_input":"2025-12-01T13:08:08.590533Z","iopub.status.idle":"2025-12-01T13:08:08.596935Z","shell.execute_reply.started":"2025-12-01T13:08:08.590504Z","shell.execute_reply":"2025-12-01T13:08:08.596334Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"num_epochs = 10\nhistory = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_dataset)\n    epoch_acc = correct_train / total_train\n    history['loss'].append(epoch_loss)\n    history['acc'].append(epoch_acc)\n\n    model.eval()\n    val_running_loss = 0.0\n    correct_val = 0\n    total_val = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            labels = labels.float().unsqueeze(1)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            val_running_loss += loss.item() * images.size(0)\n            predicted = (torch.sigmoid(outputs) > 0.5).float()\n            total_val += labels.size(0)\n            correct_val += (predicted == labels).sum().item()\n\n    val_epoch_loss = val_running_loss / len(test_dataset)\n    val_epoch_acc = correct_val / total_val\n    history['val_loss'].append(val_epoch_loss)\n    history['val_acc'].append(val_epoch_acc)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:08:10.377150Z","iopub.execute_input":"2025-12-01T13:08:10.377810Z","iopub.status.idle":"2025-12-01T13:09:40.325588Z","shell.execute_reply.started":"2025-12-01T13:08:10.377785Z","shell.execute_reply":"2025-12-01T13:09:40.324871Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss: 0.6446, Acc: 0.6392, Val Loss: 0.6039, Val Acc: 0.6617\nEpoch 2/10, Loss: 0.5448, Acc: 0.7079, Val Loss: 0.7690, Val Acc: 0.5821\nEpoch 3/10, Loss: 0.5151, Acc: 0.7266, Val Loss: 0.6815, Val Acc: 0.6070\nEpoch 4/10, Loss: 0.4696, Acc: 0.7541, Val Loss: 0.7208, Val Acc: 0.6517\nEpoch 5/10, Loss: 0.4525, Acc: 0.7840, Val Loss: 0.6366, Val Acc: 0.6766\nEpoch 6/10, Loss: 0.4034, Acc: 0.8077, Val Loss: 0.6472, Val Acc: 0.6915\nEpoch 7/10, Loss: 0.3259, Acc: 0.8602, Val Loss: 0.7483, Val Acc: 0.6816\nEpoch 8/10, Loss: 0.2576, Acc: 0.8914, Val Loss: 0.8658, Val Acc: 0.6716\nEpoch 9/10, Loss: 0.1988, Acc: 0.9313, Val Loss: 0.8272, Val Acc: 0.7015\nEpoch 10/10, Loss: 0.1624, Acc: 0.9376, Val Loss: 2.4677, Val Acc: 0.5572\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"train_loss = np.array(history['loss'])\ntrain_acc = np.array(history['acc'])\nnp.median(train_acc), np.std(train_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:10:06.416582Z","iopub.execute_input":"2025-12-01T13:10:06.417257Z","iopub.status.idle":"2025-12-01T13:10:06.424819Z","shell.execute_reply.started":"2025-12-01T13:10:06.417234Z","shell.execute_reply":"2025-12-01T13:10:06.424060Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(0.7958801498127341, 0.1497657787776018)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"train_aug_transforms = transforms.Compose([\n    transforms.Resize((200, 200)),\n    transforms.RandomRotation(50),\n    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]),\n])\ntest_transforms = transforms.Compose([\n    transforms.Resize((200, 200)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ) # ImageNet normalization\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:11:49.923889Z","iopub.execute_input":"2025-12-01T13:11:49.924205Z","iopub.status.idle":"2025-12-01T13:11:49.929807Z","shell.execute_reply.started":"2025-12-01T13:11:49.924183Z","shell.execute_reply":"2025-12-01T13:11:49.929030Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataset = HairDataset(\n    data_dir = '/kaggle/working/data/train',\n    transform=train_aug_transforms\n)\ntest_dataset = HairDataset(\n    data_dir = '/kaggle/working/data/test',\n    transform=test_transforms\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:12:17.964401Z","iopub.execute_input":"2025-12-01T13:12:17.965142Z","iopub.status.idle":"2025-12-01T13:12:17.971223Z","shell.execute_reply.started":"2025-12-01T13:12:17.965116Z","shell.execute_reply":"2025-12-01T13:12:17.970697Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"num_epochs = 10\nhistory = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_dataset)\n    epoch_acc = correct_train / total_train\n    history['loss'].append(epoch_loss)\n    history['acc'].append(epoch_acc)\n\n    model.eval()\n    val_running_loss = 0.0\n    correct_val = 0\n    total_val = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            labels = labels.float().unsqueeze(1)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            val_running_loss += loss.item() * images.size(0)\n            predicted = (torch.sigmoid(outputs) > 0.5).float()\n            total_val += labels.size(0)\n            correct_val += (predicted == labels).sum().item()\n\n    val_epoch_loss = val_running_loss / len(test_dataset)\n    val_epoch_acc = correct_val / total_val\n    history['val_loss'].append(val_epoch_loss)\n    history['val_acc'].append(val_epoch_acc)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:12:36.361277Z","iopub.execute_input":"2025-12-01T13:12:36.362079Z","iopub.status.idle":"2025-12-01T13:14:15.199774Z","shell.execute_reply.started":"2025-12-01T13:12:36.362053Z","shell.execute_reply":"2025-12-01T13:14:15.199025Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss: 0.8915, Acc: 0.6092, Val Loss: 0.8120, Val Acc: 0.5821\nEpoch 2/10, Loss: 0.5862, Acc: 0.6792, Val Loss: 0.6318, Val Acc: 0.6766\nEpoch 3/10, Loss: 0.5567, Acc: 0.7029, Val Loss: 0.5745, Val Acc: 0.6965\nEpoch 4/10, Loss: 0.5898, Acc: 0.6792, Val Loss: 0.6599, Val Acc: 0.6318\nEpoch 5/10, Loss: 0.5540, Acc: 0.7091, Val Loss: 0.5487, Val Acc: 0.7164\nEpoch 6/10, Loss: 0.5371, Acc: 0.7216, Val Loss: 0.5529, Val Acc: 0.7313\nEpoch 7/10, Loss: 0.5495, Acc: 0.7104, Val Loss: 0.5645, Val Acc: 0.6965\nEpoch 8/10, Loss: 0.5168, Acc: 0.7378, Val Loss: 0.6475, Val Acc: 0.6915\nEpoch 9/10, Loss: 0.5094, Acc: 0.7453, Val Loss: 0.5187, Val Acc: 0.7562\nEpoch 10/10, Loss: 0.4798, Acc: 0.7578, Val Loss: 0.5511, Val Acc: 0.7065\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"test_loss = np.array(history['val_loss'])\ntest_acc = np.array(history['val_acc'])\nnp.mean(test_loss), np.mean(test_acc[-5:])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:17:11.012547Z","iopub.execute_input":"2025-12-01T13:17:11.013243Z","iopub.status.idle":"2025-12-01T13:17:11.018524Z","shell.execute_reply.started":"2025-12-01T13:17:11.013214Z","shell.execute_reply":"2025-12-01T13:17:11.017914Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(0.6061646786495227, 0.7164179104477613)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}